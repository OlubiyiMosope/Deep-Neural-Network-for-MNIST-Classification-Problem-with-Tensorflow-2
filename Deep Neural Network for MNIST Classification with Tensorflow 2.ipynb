{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST Classification\n",
    "\n",
    "The objective of of this project is to write a deep neural a network algorithm that correctly detects hand written digits. This is a classification problem with 10 classes because there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9). \n",
    "\n",
    "The MNIST problem is often called the \"Hello World!\" of deep learning.\n",
    "\n",
    "The dataset is called MNIST and refers to handwritten digit recognition. The dataset is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau.  \n",
    "The 70,000 images each image have 784 features. This is because each image is 28 × 28 pixels, and each feature simply represents one pixel’s intensity, from 0 (white) to 255 (black). Each image is a single digit.\n",
    "\n",
    "The project's authors are Yann LeCun, Corinna Cortes, CJ Burges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preprocessing the data:\n",
    "Begin by downloading and loading the data with `tfds.load`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with_info=True will also provide us with a tuple containing information about the version, features, number of samples\n",
    "# as_supervised=True will load the dataset in a 2-tuple structure (input, target)\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, extract the training and testing dataset with the built references:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set aside a validation set from the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of validation samples as a % of the train samples\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "# cast this number to an integer, as a float may cause an error down the line.\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "# store the number of test samples in a dedicated variable and cast to an integer.\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the data inputs will be scaled from a value range of [0,255] to [0,1] in order to make the result more numerically stable, and is likely to improve the performance of the model.  \n",
    "A function will be defined to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function will will take an MNIST image and its label\n",
    "def scale(image, label):\n",
    "    # cast the values to float\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "\n",
    "    return image, label\n",
    "\n",
    "# map this custom transformer with the dataset using `.map()`\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "# finally, we scale test data so it has the same magnitude as the train and validation\n",
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next preprocessing step is to set a buffer size, and shuffle the train and validation data.  \n",
    "Shuffling is important because it will prevent the algorithm from assigning importance to the arrangement order of the training set, and ensure that the training instances in each batch is representative of the entire dataset, including as many of the target classes as possibe, does not just only one or a small number of the total classes.  \n",
    "The test data does not need to be shuffled as it will not be partaking in the training process.\n",
    "\n",
    "The buffers size is a hyperparameter that is set to account for cases where the dataset is enormous and shuffling might not be possible in one go because it cannot fit into the memory all at once.  \n",
    "The dataset will be loaded into the memory per time according to the buffer size. This technique also optimizes the system's performance.\n",
    "\n",
    "The train and validation data will be shuffled using `.shuffle()` and set the buffer size as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# Extract the validation data, which is 10% of the training set\n",
    "# we use the .take() method to take that many samples\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "# the train_data is everything else, so we skip as many samples as there are in the validation dataset.\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a batch size will be set. This is another hyperparameter that may need to be tuned during the training process.\n",
    "Batching involves splitting the train data into the set number of batches; training the model on each set, and updating the weights right after.\n",
    "\n",
    "We must set a batch size for the train data in order to utilise the more efficient Mini-batch Gradient Descent as against Batch Gradient Descent.  \n",
    "The weights are only updated once per batch. Updating the weights in smaller batches rather than the entire batch at once is proven to be a more efficient way to update weights.\n",
    "\n",
    "The validation data does not need to be batched because there will only be forward propagation and no backpropagation since we are only interested in the errors not updating the weights; when batching we usually find the average loss and average accuracy, but during validation and testing, we want the exact values, therefore we should take all the data at once.  \n",
    "However, we should still batch it because the model expects the validation data in batch form too. \n",
    "But here, the batch size will be set to the number of the validation samples - this will make it only a single batch. This same logic will be applied for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the batch size\n",
    "BATCH_SIZE = 100\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples)  # will have only one batch.\n",
    "\n",
    "# batch the test data\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "# split 2-tuple structure of the validation data to separate the input features from the targets.\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the input and output sizes\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "\n",
    "# define hidden layer size.\n",
    "hidden_layer_size = 200\n",
    "    \n",
    "# outline the model\n",
    "model = tf.keras.Sequential([\n",
    "    # Flatten the array to a (784,) vector.\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer\n",
    "    \n",
    "    # model the hidden layers of the network\n",
    "    # set the hidden layer sizes (width of the network), and activation function to be used.\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 3rd hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 4th hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 5th hidden layer\n",
    "    \n",
    "    \n",
    "    # define the output layer and define its activation function. \n",
    "    # Softmax is the most suitable because we are dealing with a classification problem.\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the optimizer and the loss function\n",
    "Define the optimizer to use (here we'll choose the 'Adaptive Moment Estimation optimizer (ADAM)'), the loss function ('sparse categorical cross entropy' - which will apply one-hot encoding for us since the targets are one-hot encoded), and the model evaluation metric ('accuracy' - which models the accuracy of the model's predictions) that we are interested in obtaining at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "That's where we train the model we have built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 5s - loss: 0.2731 - accuracy: 0.9158 - val_loss: 0.1410 - val_accuracy: 0.9568\n",
      "Epoch 2/5\n",
      "540/540 - 7s - loss: 0.1080 - accuracy: 0.9674 - val_loss: 0.0840 - val_accuracy: 0.9758\n",
      "Epoch 3/5\n",
      "540/540 - 7s - loss: 0.0763 - accuracy: 0.9769 - val_loss: 0.0700 - val_accuracy: 0.9793\n",
      "Epoch 4/5\n",
      "540/540 - 6s - loss: 0.0622 - accuracy: 0.9808 - val_loss: 0.0728 - val_accuracy: 0.9780\n",
      "Epoch 5/5\n",
      "540/540 - 7s - loss: 0.0515 - accuracy: 0.9835 - val_loss: 0.0611 - val_accuracy: 0.9818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c076de28e0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the maximum number of epochs\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Fit the model to the data.\n",
    "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model gives us an accuracy of 98% on the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "The final step is to test the trained model on data that it has not 'seen' before. Fiddling with the hyperparameters introduces the risk of the model overfitting the validation dataset. Therefore, it is important to evaluate the model on unseen data as it measures the true performance of the model and help to expose whether or not the model has overfit the training data.  \n",
    "\n",
    "The evaluated performance of the model on the test data can be viewed as the true pointer of the model's performance in deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 0s/step - loss: 0.0964 - accuracy: 0.9721\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.10. Test accuracy: 97.21%\n"
     ]
    }
   ],
   "source": [
    "# Presenting the result in a nicely formatted way.\n",
    "print(f'Test loss: {test_loss:.2f}. Test accuracy: {(test_accuracy*100.):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "We have successfully trained a deep neural network that accurately predicts hand-written digits with a 97.21% accuracy.  \n",
    "\n",
    "This was achieved by outlining a deep learning model with an input size of (781,), five (5) hidden layers each with a width size of 200, a 'Rectified Linear Unit (RELU)' activation function, an output layer of size 10, for each of the digit classes, and a 'Softmax' activation function - which is the preferred function for the output layer of classification problems.  \n",
    "The model was compiled using the 'Adaptive Moment Estimation (ADAM)' optimizer, a 'Sparse categorical Entropy' loss function, and an evaluation metric of 'Accuracy'.\n",
    "\n",
    "The model was fit to the training data, trained in 5 epochs, and validated on the validation data. The validation accuracy was calculated to be 98.18% and when tested on the test data produced an accuracy of 97.21%. These are similar numbers therefore it proves that the model did not overfit the train data, and will be expected to perform as well in deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
